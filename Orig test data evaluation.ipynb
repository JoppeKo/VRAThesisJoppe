{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afaa7973-726c-4af3-81fc-bd80bea147e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b93580fc-51e1-44b1-97d7-1d2f6a76ed7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(\"hpc_space/Results/MBERT/MBERT_augmented_predictions_5.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "78f6f115-2218-45b5-943c-cf57ac5c740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = predictions.iloc[:len(predictions) // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e80995a7-9453-4d8b-a94f-d0dbfa8d40b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Overall): 0.9054\n",
      "Precision (Positive): 0.5135\n",
      "Recall (Positive): 0.2676\n",
      "F1 Score (Positive): 0.3519\n",
      "Precision (Negative): 0.9260\n",
      "Recall (Negative): 0.9731\n",
      "F1 Score (Negative): 0.9490\n",
      "AUC: 0.6203\n",
      "AUC (Male): 0.6423\n",
      "AUC (Female): 0.5239\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Calculate accuracy for the positive class (label = 1)\n",
    "accuracy_positive = accuracy_score(final_df['label'], final_df['final_prediction'])\n",
    "\n",
    "# Calculate precision for the positive class (label = 1)\n",
    "precision_positive = precision_score(final_df['label'], final_df['final_prediction'], pos_label=1)\n",
    "\n",
    "# Calculate recall for the positive class (label = 1)\n",
    "recall_positive = recall_score(final_df['label'], final_df['final_prediction'], pos_label=1)\n",
    "\n",
    "# Calculate F1 score for the positive class (label = 1)\n",
    "f1_positive = f1_score(final_df['label'], final_df['final_prediction'], pos_label=1)\n",
    "\n",
    "# Calculate precision for the negative class (label = 0)\n",
    "precision_negative = precision_score(final_df['label'], final_df['final_prediction'], pos_label=0)\n",
    "\n",
    "# Calculate recall for the negative class (label = 0)\n",
    "recall_negative = recall_score(final_df['label'], final_df['final_prediction'], pos_label=0)\n",
    "\n",
    "# Calculate F1 score for the negative class (label = 0)\n",
    "f1_negative = f1_score(final_df['label'], final_df['final_prediction'], pos_label=0)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(final_df['label'], final_df['final_prediction'])\n",
    "\n",
    "# Calculate the AUC for males (Geslacht = 1)\n",
    "auc_male = roc_auc_score(final_df[final_df['Geslacht'] == 1]['label'], final_df[final_df['Geslacht'] == 1]['final_prediction'])\n",
    "\n",
    "# Calculate the AUC for females (Geslacht = 0)\n",
    "auc_female = roc_auc_score(final_df[final_df['Geslacht'] == 0]['label'], final_df[final_df['Geslacht'] == 0]['final_prediction'])\n",
    "\n",
    "\n",
    "# Print the calculated metrics separately for both classes\n",
    "print(f\"Accuracy (Overall): {accuracy_positive:.4f}\")\n",
    "print(f\"Precision (Positive): {precision_positive:.4f}\")\n",
    "print(f\"Recall (Positive): {recall_positive:.4f}\")\n",
    "print(f\"F1 Score (Positive): {f1_positive:.4f}\")\n",
    "print(f\"Precision (Negative): {precision_negative:.4f}\")\n",
    "print(f\"Recall (Negative): {recall_negative:.4f}\")\n",
    "print(f\"F1 Score (Negative): {f1_negative:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"AUC (Male): {auc_male:.4f}\")\n",
    "print(f\"AUC (Female): {auc_female:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "725afc9e-0871-4221-b06a-884af0159cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TPR (Male): 0.3158\n",
      "TPR (Female): 0.0714\n",
      "FPR (Male): 0.0312\n",
      "FPR (Female): 0.0236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix for the entire dataset\n",
    "cm = confusion_matrix(final_df['label'], final_df['final_prediction'])\n",
    "\n",
    "# Calculate TPR and FPR for male (Geslacht = 1)\n",
    "male_indices = final_df['Geslacht'] == 1\n",
    "cm_male = confusion_matrix(final_df[male_indices]['label'], final_df[male_indices]['final_prediction'])\n",
    "\n",
    "tpr_male = cm_male[1, 1] / (cm_male[1, 0] + cm_male[1, 1])\n",
    "fpr_male = cm_male[0, 1] / (cm_male[0, 0] + cm_male[0, 1])\n",
    "\n",
    "# Calculate TPR and FPR for female (Geslacht = 0)\n",
    "female_indices = final_df['Geslacht'] == 0\n",
    "cm_female = confusion_matrix(final_df[female_indices]['label'], final_df[female_indices]['final_prediction'])\n",
    "\n",
    "tpr_female = cm_female[1, 1] / (cm_female[1, 0] + cm_female[1, 1])\n",
    "fpr_female = cm_female[0, 1] / (cm_female[0, 0] + cm_female[0, 1])\n",
    "\n",
    "# Print the calculated metrics separately for both classes\n",
    "print(f\"TPR (Male): {tpr_male:.4f}\")\n",
    "print(f\"TPR (Female): {tpr_female:.4f}\")\n",
    "print(f\"FPR (Male): {fpr_male:.4f}\")\n",
    "print(f\"FPR (Female): {fpr_female:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39cd04c3-0b0c-450b-8a3a-7f6d55bdc7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Counts:\n",
      "combined\n",
      "0_0_0    372\n",
      "1_0_0    279\n",
      "1_1_0     39\n",
      "1_1_1     18\n",
      "0_1_0     13\n",
      "0_0_1      9\n",
      "1_0_1      9\n",
      "0_1_1      1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/11299035/ipykernel_795128/3127719702.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['combined'] = final_df['Geslacht'].astype(str) + '_' + final_df['label'].astype(str) + '_' + final_df['final_prediction'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "final_df['combined'] = final_df['Geslacht'].astype(str) + '_' + final_df['label'].astype(str) + '_' + final_df['final_prediction'].astype(str)\n",
    "\n",
    "# Get the count of combinations\n",
    "combination_counts = final_df['combined'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Combined Counts:\")\n",
    "print(combination_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c686e-72c7-4282-a5fd-9bc62dca9fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480478b0-8748-4cd8-8e69-8d5628edfe9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
