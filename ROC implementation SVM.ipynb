{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483e6ff1-502d-4e31-b6a3-62e711c8d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from warnings import warn\n",
    "\n",
    "from aif360.algorithms import Transformer\n",
    "from aif360.metrics import utils\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982e37b8-7e30-4bdc-925f-5787062c6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RejectOptionClassification(Transformer):\n",
    "\n",
    "    \"\"\"Reject option classification is a postprocessing technique that gives\n",
    "    favorable outcomes to unpriviliged groups and unfavorable outcomes to\n",
    "    priviliged groups in a confidence band around the decision boundary with the\n",
    "    highest uncertainty [10]_.\n",
    "\n",
    "    References:\n",
    "        .. [10] F. Kamiran, A. Karim, and X. Zhang, \"Decision Theory for\n",
    "           Discrimination-Aware Classification,\" IEEE International Conference\n",
    "           on Data Mining, 2012.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, unprivileged_groups, privileged_groups,\n",
    "                low_class_thresh=0.01, high_class_thresh=0.99,\n",
    "                num_class_thresh=100, num_ROC_margin=50,\n",
    "                metric_name=\"Statistical parity difference\",\n",
    "                metric_ub=0.05, metric_lb=-0.05):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            unprivileged_groups (dict or list(dict)): Representation for\n",
    "                unprivileged group.\n",
    "            privileged_groups (dict or list(dict)): Representation for\n",
    "                privileged group.\n",
    "            low_class_thresh (float): Smallest classification threshold to use\n",
    "                in the optimization. Should be between 0. and 1.\n",
    "            high_class_thresh (float): Highest classification threshold to use\n",
    "                in the optimization. Should be between 0. and 1.\n",
    "            num_class_thresh (int): Number of classification thresholds between\n",
    "                low_class_thresh and high_class_thresh for the optimization\n",
    "                search. Should be > 0.\n",
    "            num_ROC_margin (int): Number of relevant ROC margins to be used in\n",
    "                the optimization search. Should be > 0.\n",
    "            metric_name (str): Name of the metric to use for the optimization.\n",
    "                Allowed options are \"Statistical parity difference\",\n",
    "                \"Average odds difference\", \"Equal opportunity difference\".\n",
    "            metric_ub (float): Upper bound of constraint on the metric value\n",
    "            metric_lb (float): Lower bound of constraint on the metric value\n",
    "        \"\"\"\n",
    "        super(RejectOptionClassification, self).__init__(\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups,\n",
    "            low_class_thresh=low_class_thresh, high_class_thresh=high_class_thresh,\n",
    "            num_class_thresh=num_class_thresh, num_ROC_margin=num_ROC_margin,\n",
    "            metric_name=metric_name)\n",
    "\n",
    "        allowed_metrics = [\"Statistical parity difference\",\n",
    "                           \"Average odds difference\",\n",
    "                           \"Equal opportunity difference\"]\n",
    "\n",
    "        self.unprivileged_groups = unprivileged_groups\n",
    "        self.privileged_groups = privileged_groups\n",
    "\n",
    "        self.low_class_thresh = low_class_thresh\n",
    "        self.high_class_thresh = high_class_thresh\n",
    "        self.num_class_thresh = num_class_thresh\n",
    "        self.num_ROC_margin = num_ROC_margin\n",
    "        self.metric_name = metric_name\n",
    "        self.metric_ub = metric_ub\n",
    "        self.metric_lb = metric_lb\n",
    "\n",
    "        self.classification_threshold = None\n",
    "        self.ROC_margin = None\n",
    "\n",
    "        if ((self.low_class_thresh < 0.0) or (self.low_class_thresh > 1.0) or\\\n",
    "            (self.high_class_thresh < 0.0) or (self.high_class_thresh > 1.0) or\\\n",
    "            (self.low_class_thresh >= self.high_class_thresh) or\\\n",
    "            (self.num_class_thresh < 1) or (self.num_ROC_margin < 1)):\n",
    "\n",
    "            raise ValueError(\"Input parameter values out of bounds\")\n",
    "\n",
    "        if metric_name not in allowed_metrics:\n",
    "            raise ValueError(\"metric name not in the list of allowed metrics\")\n",
    "\n",
    "    def fit(self, dataset_true, dataset_pred):\n",
    "        \"\"\"Estimates the optimal classification threshold and margin for reject\n",
    "        option classification that optimizes the metric provided.\n",
    "\n",
    "        Note:\n",
    "            The `fit` function is a no-op for this algorithm.\n",
    "\n",
    "        Args:\n",
    "            dataset_true (BinaryLabelDataset): Dataset containing the true\n",
    "                `labels`.\n",
    "            dataset_pred (BinaryLabelDataset): Dataset containing the predicted\n",
    "                `scores`.\n",
    "\n",
    "        Returns:\n",
    "            RejectOptionClassification: Returns self.\n",
    "        \"\"\"\n",
    "\n",
    "        fair_metric_arr = np.zeros(self.num_class_thresh*self.num_ROC_margin)\n",
    "        balanced_acc_arr = np.zeros_like(fair_metric_arr)\n",
    "        ROC_margin_arr = np.zeros_like(fair_metric_arr)\n",
    "        class_thresh_arr = np.zeros_like(fair_metric_arr)\n",
    "\n",
    "        cnt = 0\n",
    "        # Iterate through class thresholds\n",
    "        for class_thresh in np.linspace(self.low_class_thresh,\n",
    "                                        self.high_class_thresh,\n",
    "                                        self.num_class_thresh):\n",
    "\n",
    "            self.classification_threshold = class_thresh\n",
    "            if class_thresh <= 0.5:\n",
    "                low_ROC_margin = 0.0\n",
    "                high_ROC_margin = class_thresh\n",
    "            else:\n",
    "                low_ROC_margin = 0.0\n",
    "                high_ROC_margin = (1.0-class_thresh)\n",
    "\n",
    "            # Iterate through ROC margins\n",
    "            for ROC_margin in np.linspace(\n",
    "                                low_ROC_margin,\n",
    "                                high_ROC_margin,\n",
    "                                self.num_ROC_margin):\n",
    "                self.ROC_margin = ROC_margin\n",
    "\n",
    "                # Predict using the current threshold and margin\n",
    "                dataset_transf_pred = self.predict(dataset_pred)\n",
    "\n",
    "                dataset_transf_metric_pred = BinaryLabelDatasetMetric(\n",
    "                                             dataset_transf_pred,\n",
    "                                             unprivileged_groups=self.unprivileged_groups,\n",
    "                                             privileged_groups=self.privileged_groups)\n",
    "                classified_transf_metric = ClassificationMetric(\n",
    "                                             dataset_true,\n",
    "                                             dataset_transf_pred,\n",
    "                                             unprivileged_groups=self.unprivileged_groups,\n",
    "                                             privileged_groups=self.privileged_groups)\n",
    "\n",
    "                ROC_margin_arr[cnt] = self.ROC_margin\n",
    "                class_thresh_arr[cnt] = self.classification_threshold\n",
    "\n",
    "                # Balanced accuracy and fairness metric computations\n",
    "                balanced_acc_arr[cnt] = 0.5*(classified_transf_metric.true_positive_rate()\\\n",
    "                                       +classified_transf_metric.true_negative_rate())\n",
    "                if self.metric_name == \"Statistical parity difference\":\n",
    "                    fair_metric_arr[cnt] = dataset_transf_metric_pred.mean_difference()\n",
    "                elif self.metric_name == \"Average odds difference\":\n",
    "                    fair_metric_arr[cnt] = classified_transf_metric.average_odds_difference()\n",
    "                elif self.metric_name == \"Equal opportunity difference\":\n",
    "                    fair_metric_arr[cnt] = classified_transf_metric.equal_opportunity_difference()\n",
    "\n",
    "                cnt += 1\n",
    "\n",
    "        rel_inds = np.logical_and(fair_metric_arr >= self.metric_lb,\n",
    "                                  fair_metric_arr <= self.metric_ub)\n",
    "        if any(rel_inds):\n",
    "            best_ind = np.where(balanced_acc_arr[rel_inds]\n",
    "                                == np.max(balanced_acc_arr[rel_inds]))[0][0]\n",
    "        else:\n",
    "            warn(\"Unable to satisy fairness constraints\")\n",
    "            rel_inds = np.ones(len(fair_metric_arr), dtype=bool)\n",
    "            best_ind = np.where(fair_metric_arr[rel_inds]\n",
    "                                == np.min(fair_metric_arr[rel_inds]))[0][0]\n",
    "\n",
    "        self.ROC_margin = ROC_margin_arr[rel_inds][best_ind]\n",
    "        self.classification_threshold = class_thresh_arr[rel_inds][best_ind]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"Obtain fair predictions using the ROC method.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing scores that will\n",
    "                be used to compute predicted labels.\n",
    "\n",
    "        Returns:\n",
    "            dataset_pred (BinaryLabelDataset): Output dataset with potentially\n",
    "            fair predictions obtain using the ROC method.\n",
    "        \"\"\"\n",
    "        dataset_new = dataset.copy(deepcopy=False)\n",
    "\n",
    "        fav_pred_inds = (dataset.scores > self.classification_threshold)\n",
    "        unfav_pred_inds = ~fav_pred_inds\n",
    "\n",
    "        y_pred = np.zeros(dataset.scores.shape)\n",
    "        y_pred[fav_pred_inds] = dataset.favorable_label\n",
    "        y_pred[unfav_pred_inds] = dataset.unfavorable_label\n",
    "\n",
    "        # Indices of critical region around the classification boundary\n",
    "        crit_region_inds = np.logical_and(\n",
    "                dataset.scores <= self.classification_threshold+self.ROC_margin,\n",
    "                dataset.scores > self.classification_threshold-self.ROC_margin)\n",
    "\n",
    "        # Indices of privileged and unprivileged groups\n",
    "        cond_priv = utils.compute_boolean_conditioning_vector(\n",
    "                        dataset.protected_attributes,\n",
    "                        dataset.protected_attribute_names,\n",
    "                        self.privileged_groups)\n",
    "        cond_unpriv = utils.compute_boolean_conditioning_vector(\n",
    "                        dataset.protected_attributes,\n",
    "                        dataset.protected_attribute_names,\n",
    "                        self.unprivileged_groups)\n",
    "\n",
    "        # New, fairer labels\n",
    "        dataset_new.labels = y_pred\n",
    "        dataset_new.labels[np.logical_and(crit_region_inds,\n",
    "                            cond_priv.reshape(-1,1))] = dataset.unfavorable_label\n",
    "        dataset_new.labels[np.logical_and(crit_region_inds,\n",
    "                            cond_unpriv.reshape(-1,1))] = dataset.favorable_label\n",
    "\n",
    "        return dataset_new\n",
    "\n",
    "    def fit_predict(self, dataset_true, dataset_pred):\n",
    "        \"\"\"fit and predict methods sequentially.\"\"\"\n",
    "        return self.fit(dataset_true, dataset_pred).predict(dataset_pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3711b6b0-d123-4906-a72a-494ca689bf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain the pareto frontier\n",
    "def _get_pareto_frontier(scores, return_mask = True):  # <- Fastest for many points\n",
    "    \"\"\"\n",
    "    :param scores: An (n_points, n_scores) array\n",
    "    :param return_mask: True to return a mask, False to return integer indices of efficient points.\n",
    "    :return: An array of indices of pareto-efficient points.\n",
    "        If return_mask is True, this will be an (n_points, ) boolean array\n",
    "        Otherwise it will be a (n_efficient_points, ) integer array of indices.\n",
    "\n",
    "    adapted from: https://stackoverflow.com/questions/32791911/fast-calculation-of-pareto-front-in-python\n",
    "    \"\"\"\n",
    "    is_efficient = np.arange(scores.shape[0])\n",
    "    n_points = scores.shape[0]\n",
    "    next_point_index = 0  # Next index in the is_efficient array to search for\n",
    "\n",
    "    while next_point_index<len(scores):\n",
    "        nondominated_point_mask = np.any(scores>=scores[next_point_index], axis=1)\n",
    "        is_efficient = is_efficient[nondominated_point_mask]  # Remove dominated points\n",
    "        scores = scores[nondominated_point_mask]\n",
    "        next_point_index = np.sum(nondominated_point_mask[:next_point_index])+1\n",
    "\n",
    "    if return_mask:\n",
    "        is_efficient_mask = np.zeros(n_points, dtype = bool)\n",
    "        is_efficient_mask[is_efficient] = True\n",
    "        return is_efficient_mask\n",
    "    else:\n",
    "        return is_efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63fc44e-066d-4d9a-b3f8-64dd9b450ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee7e50-4eb8-404f-9677-0b05a6c772db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_df = pd.read_csv('hpc_space/Results/NEW RESULTS/NEW RESULTS/Doc2Vec SVM/Orig/SVMDoc2Vec5_augmentedtest_predictions.csv', sep=';')\n",
    "validation_predictions_df = pd.read_csv('hpc_space/Results/NEW RESULTS/NEW RESULTS/Doc2Vec SVM/Orig/SVMDoc2Vec_5_val_predictions.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfb1697-f571-4adf-8d89-7c1a83215b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225d884-2ef1-4318-9b88-98161a07f9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Calculate accuracy for the positive class (label = 1)\n",
    "accuracy_positive = accuracy_score(validation_predictions_df['label'], validation_predictions_df['final_prediction'])\n",
    "\n",
    "# Calculate precision for the positive class (label = 1)\n",
    "precision_positive = precision_score(validation_predictions_df['label'], validation_predictions_df['final_prediction'], pos_label=1)\n",
    "\n",
    "# Calculate recall for the positive class (label = 1)\n",
    "recall_positive = recall_score(validation_predictions_df['label'], validation_predictions_df['final_prediction'], pos_label=1)\n",
    "\n",
    "# Calculate F1 score for the positive class (label = 1)\n",
    "f1_positive = f1_score(validation_predictions_df['label'], validation_predictions_df['final_prediction'], pos_label=1)\n",
    "\n",
    "# Calculate precision for the negative class (label = 0)\n",
    "precision_negative = precision_score(validation_predictions_df['label'], validation_predictions_df['final_prediction'], pos_label=0)\n",
    "\n",
    "# Calculate recall for the negative class (label = 0)\n",
    "recall_negative = recall_score(validation_predictions_df['label'], validation_predictions_df['final_prediction'], pos_label=0)\n",
    "\n",
    "# Calculate F1 score for the negative class (label = 0)\n",
    "f1_negative = f1_score(validation_predictions_df['label'], validation_predictions_df['final_prediction'], pos_label=0)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(validation_predictions_df['label'], validation_predictions_df['final_prediction'])\n",
    "\n",
    "# Calculate the AUC for males (Geslacht = 1)\n",
    "auc_male = roc_auc_score(validation_predictions_df[validation_predictions_df['Geslacht'] == 1]['label'], validation_predictions_df[validation_predictions_df['Geslacht'] == 1]['final_prediction'])\n",
    "\n",
    "# Calculate the AUC for females (Geslacht = 0)\n",
    "auc_female = roc_auc_score(validation_predictions_df[validation_predictions_df['Geslacht'] == 0]['label'], validation_predictions_df[validation_predictions_df['Geslacht'] == 0]['final_prediction'])\n",
    "\n",
    "\n",
    "# Print the calculated metrics separately for both classes\n",
    "print(f\"Accuracy (Overall): {accuracy_positive:.4f}\")\n",
    "print(f\"Precision (Positive): {precision_positive:.4f}\")\n",
    "print(f\"Recall (Positive): {recall_positive:.4f}\")\n",
    "print(f\"F1 Score (Positive): {f1_positive:.4f}\")\n",
    "print(f\"Precision (Negative): {precision_negative:.4f}\")\n",
    "print(f\"Recall (Negative): {recall_negative:.4f}\")\n",
    "print(f\"F1 Score (Negative): {f1_negative:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"AUC (Male): {auc_male:.4f}\")\n",
    "print(f\"AUC (Female): {auc_female:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffc201-49bf-4431-b998-1a0b79a52899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix for the entire dataset\n",
    "cm = confusion_matrix(validation_predictions_df['label'], validation_predictions_df['final_prediction'])\n",
    "\n",
    "# Calculate TPR and FPR for male (Geslacht = 1)\n",
    "male_indices = validation_predictions_df['Geslacht'] == 1\n",
    "cm_male = confusion_matrix(validation_predictions_df[male_indices]['label'], validation_predictions_df[male_indices]['final_prediction'])\n",
    "\n",
    "tpr_male = cm_male[1, 1] / (cm_male[1, 0] + cm_male[1, 1])\n",
    "fpr_male = cm_male[0, 1] / (cm_male[0, 0] + cm_male[0, 1])\n",
    "\n",
    "# Calculate TPR and FPR for female (Geslacht = 0)\n",
    "female_indices = validation_predictions_df['Geslacht'] == 0\n",
    "cm_female = confusion_matrix(validation_predictions_df[female_indices]['label'], validation_predictions_df[female_indices]['final_prediction'])\n",
    "\n",
    "tpr_female = cm_female[1, 1] / (cm_female[1, 0] + cm_female[1, 1])\n",
    "fpr_female = cm_female[0, 1] / (cm_female[0, 0] + cm_female[0, 1])\n",
    "\n",
    "# Print the calculated metrics separately for both classes\n",
    "print(f\"TPR (Male): {tpr_male:.4f}\")\n",
    "print(f\"TPR (Female): {tpr_female:.4f}\")\n",
    "print(f\"FPR (Male): {fpr_male:.4f}\")\n",
    "print(f\"FPR (Female): {fpr_female:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc945b-311f-4abb-a7ba-04f468360cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aed99f-1c87-44f4-8a56-3a5456b6cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fed1914-755d-4d35-8e70-6bfb06c81c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df = pd.DataFrame({\n",
    "    'label': validation_predictions_df['label'],\n",
    "    'Geslacht': validation_predictions_df['Geslacht']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eeeacf5-7058-4642-a104-83920c8119b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame({\n",
    "    'label': validation_predictions_df['final_prediction'],\n",
    "    'predicted_probability': validation_predictions_df['predicted_probabilities'],\n",
    "    'Geslacht': validation_predictions_df['Geslacht']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c21ff4-6b43-4745-8451-ee8f9cc011b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0d4e04-9aed-41ac-a13a-7c26c47bda6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_protected_attributes = [0]  # 1 is male\n",
    "unprivileged_protected_attributes = [1]  # 0 is female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3302390-b91a-49c3-be19-9313b3e9605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "favorable_label = 1\n",
    "unfavorable_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76c7c06-22d5-4b89-acbb-8f0483b42212",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute_names = ['Geslacht']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfddb8b-398b-45ad-afc1-fc9023601571",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_true_validation = BinaryLabelDataset(\n",
    "    favorable_label=favorable_label,\n",
    "    unfavorable_label=unfavorable_label,\n",
    "    df=ground_truth_df,\n",
    "    label_names=['label'],\n",
    "    protected_attribute_names=['Geslacht'],\n",
    "    unprivileged_protected_attributes=[[0]],\n",
    "    privileged_protected_attributes=[[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99c9e7-8c8b-4c3d-a72c-7b365f845b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_true_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf0ebb0-e8d5-4ac9-a5af-561ac78f281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred_validation = BinaryLabelDataset(\n",
    "    favorable_label=favorable_label,\n",
    "    unfavorable_label=unfavorable_label,\n",
    "    df=predictions_df,\n",
    "    label_names=['label'],\n",
    "    scores_names=['predicted_probability'],\n",
    "    protected_attribute_names=['Geslacht'],\n",
    "    unprivileged_protected_attributes=[[0]],\n",
    "    privileged_protected_attributes=[[1]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736f0b41-6f52-466a-bc79-bed9196429cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_pred_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40565c58-22c4-4422-94b5-d59f06c390ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc = RejectOptionClassification(\n",
    "    unprivileged_groups=[{'Geslacht': 0}],  # Replace with your unprivileged group representation\n",
    "    privileged_groups=[{'Geslacht': 1}],    # Replace with your privileged group representation\n",
    "    low_class_thresh=0.01,\n",
    "    high_class_thresh=0.99,\n",
    "    num_class_thresh=100,\n",
    "    num_ROC_margin=50,\n",
    "    metric_name=\"Average odds difference\",\n",
    "    metric_ub=0.05,\n",
    "    metric_lb=-0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aeb0ad-1bf2-43ab-9a78-818e26e2ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc.fit_predict(dataset_true_validation, dataset_pred_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87689fd6-e65e-4492-9f03-f2d162d20777",
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_metric = ClassificationMetric(dataset_true_validation, roc.predict(dataset_pred_validation),\n",
    "                                       unprivileged_groups=[{'Geslacht': 0}],\n",
    "                                       privileged_groups=[{'Geslacht': 1}])\n",
    "print(\"Average Odds Difference:\", fairness_metric.average_odds_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841b46a-df0e-47d2-ae53-a20a6d8734ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions = roc.predict(dataset_pred_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb7103-52f3-46f6-adb3-924702fc34a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten multi-dimensional arrays\n",
    "labels_flat = new_predictions.labels.flatten()\n",
    "scores_flat = new_predictions.scores.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020b795-2f4a-4b9a-a76b-3ced91cd3598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels from the original dataset\n",
    "true_labels = dataset_true_validation.labels.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb2778-9516-4371-a2a6-5a75d9f2e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions_df = pd.DataFrame({\n",
    "    'predicted_label': labels_flat, \n",
    "    'predicted_probabilities': scores_flat,\n",
    "    'label': true_labels,\n",
    "    'Geslacht': validation_predictions_df['Geslacht'],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa7235-6aa8-44ed-a6b9-fc7463f455d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Calculate accuracy for the positive class (label = 1)\n",
    "accuracy_positive = accuracy_score(new_predictions_df['label'], new_predictions_df['predicted_label'])\n",
    "\n",
    "# Calculate precision for the positive class (label = 1)\n",
    "precision_positive = precision_score(new_predictions_df['label'], new_predictions_df['predicted_label'], pos_label=1)\n",
    "\n",
    "# Calculate recall for the positive class (label = 1)\n",
    "recall_positive = recall_score(new_predictions_df['label'], new_predictions_df['predicted_label'], pos_label=1)\n",
    "\n",
    "# Calculate F1 score for the positive class (label = 1)\n",
    "f1_positive = f1_score(new_predictions_df['label'], new_predictions_df['predicted_label'], pos_label=1)\n",
    "\n",
    "# Calculate precision for the negative class (label = 0)\n",
    "precision_negative = precision_score(new_predictions_df['label'], new_predictions_df['predicted_label'], pos_label=0)\n",
    "\n",
    "# Calculate recall for the negative class (label = 0)\n",
    "recall_negative = recall_score(new_predictions_df['label'], new_predictions_df['predicted_label'], pos_label=0)\n",
    "\n",
    "# Calculate F1 score for the negative class (label = 0)\n",
    "f1_negative = f1_score(new_predictions_df['label'], new_predictions_df['predicted_label'], pos_label=0)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(new_predictions_df['label'], new_predictions_df['predicted_label'])\n",
    "\n",
    "# Calculate the AUC for males (Geslacht = 1)\n",
    "auc_male = roc_auc_score(new_predictions_df[new_predictions_df['Geslacht'] == 1]['label'], new_predictions_df[new_predictions_df['Geslacht'] == 1]['predicted_label'])\n",
    "\n",
    "# Calculate the AUC for females (Geslacht = 0)\n",
    "auc_female = roc_auc_score(new_predictions_df[new_predictions_df['Geslacht'] == 0]['label'], new_predictions_df[new_predictions_df['Geslacht'] == 0]['predicted_label'])\n",
    "\n",
    "\n",
    "# Print the calculated metrics separately for both classes\n",
    "print(f\"Accuracy (Overall): {accuracy_positive:.4f}\")\n",
    "print(f\"Precision (Positive): {precision_positive:.4f}\")\n",
    "print(f\"Recall (Positive): {recall_positive:.4f}\")\n",
    "print(f\"F1 Score (Positive): {f1_positive:.4f}\")\n",
    "print(f\"Precision (Negative): {precision_negative:.4f}\")\n",
    "print(f\"Recall (Negative): {recall_negative:.4f}\")\n",
    "print(f\"F1 Score (Negative): {f1_negative:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"AUC (Male): {auc_male:.4f}\")\n",
    "print(f\"AUC (Female): {auc_female:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f79090c-103f-444a-8c0a-064e5f4340af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix for the entire dataset\n",
    "cm = confusion_matrix(new_predictions_df['label'], new_predictions_df['predicted_label'])\n",
    "\n",
    "# Calculate TPR and FPR for male (Geslacht = 1)\n",
    "male_indices = new_predictions_df['Geslacht'] == 1\n",
    "cm_male = confusion_matrix(new_predictions_df[male_indices]['label'], new_predictions_df[male_indices]['predicted_label'])\n",
    "\n",
    "tpr_male = cm_male[1, 1] / (cm_male[1, 0] + cm_male[1, 1])\n",
    "fpr_male = cm_male[0, 1] / (cm_male[0, 0] + cm_male[0, 1])\n",
    "\n",
    "# Calculate TPR and FPR for female (Geslacht = 0)\n",
    "female_indices = new_predictions_df['Geslacht'] == 0\n",
    "cm_female = confusion_matrix(new_predictions_df[female_indices]['label'], new_predictions_df[female_indices]['predicted_label'])\n",
    "\n",
    "tpr_female = cm_female[1, 1] / (cm_female[1, 0] + cm_female[1, 1])\n",
    "fpr_female = cm_female[0, 1] / (cm_female[0, 0] + cm_female[0, 1])\n",
    "\n",
    "# Print the calculated metrics separately for both classes\n",
    "print(f\"TPR (Male): {tpr_male:.4f}\")\n",
    "print(f\"TPR (Female): {tpr_female:.4f}\")\n",
    "print(f\"FPR (Male): {fpr_male:.4f}\")\n",
    "print(f\"FPR (Female): {fpr_female:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0e2ba-2425-485c-b340-24b8d0b985bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = pd.DataFrame({\n",
    "    'label': test_predictions_df['final_prediction'],\n",
    "    'predicted_probability': test_predictions_df['predicted_probabilities'],\n",
    "    'Geslacht': test_predictions_df['Geslacht']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbbe293-6dbd-4568-8ac2-04a44f521c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pred_test = BinaryLabelDataset(\n",
    "    favorable_label=favorable_label,\n",
    "    unfavorable_label=unfavorable_label,\n",
    "    df=predictions_test,\n",
    "    label_names=['label'],\n",
    "    protected_attribute_names=['Geslacht'],\n",
    "    unprivileged_protected_attributes=[[0]],\n",
    "    privileged_protected_attributes=[[1]],\n",
    "    scores_names=['predicted_probability'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf5400-da7d-46c1-bcfa-e8fe1312230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_predictions_test = roc.predict(dataset_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac7d9a-4910-4c2d-b6cd-182e2e29e88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten multi-dimensional arrays\n",
    "labels_flat_test = new_predictions_test.labels.flatten()\n",
    "scores_flat_test = new_predictions_test.scores.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728fe38-ab97-4519-8bf5-0ce22ccd5bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_predictions_df = pd.DataFrame({\n",
    "    'predicted_label': labels_flat_test, \n",
    "    'predicted_probabilities': scores_flat_test,\n",
    "    'label': test_predictions_df['label'],\n",
    "    'Geslacht': test_predictions_df['Geslacht'],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a3660-baea-4dd3-8b73-66ecbdf8edc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Calculate accuracy for the positive class (label = 1)\n",
    "accuracy_positive = accuracy_score(new_test_predictions_df['label'], new_test_predictions_df['predicted_label'])\n",
    "\n",
    "# Calculate precision for the positive class (label = 1)\n",
    "precision_positive = precision_score(new_test_predictions_df['label'], new_test_predictions_df['predicted_label'], pos_label=1)\n",
    "\n",
    "# Calculate recall for the positive class (label = 1)\n",
    "recall_positive = recall_score(new_test_predictions_df['label'], new_test_predictions_df['predicted_label'], pos_label=1)\n",
    "\n",
    "# Calculate F1 score for the positive class (label = 1)\n",
    "f1_positive = f1_score(new_test_predictions_df['label'], new_test_predictions_df['predicted_label'], pos_label=1)\n",
    "\n",
    "# Calculate precision for the negative class (label = 0)\n",
    "precision_negative = precision_score(new_test_predictions_df['label'], new_test_predictions_df['predicted_label'], pos_label=0)\n",
    "\n",
    "# Calculate recall for the negative class (label = 0)\n",
    "recall_negative = recall_score(new_test_predictions_df['label'], new_test_predictions_df['predicted_label'], pos_label=0)\n",
    "\n",
    "# Calculate F1 score for the negative class (label = 0)\n",
    "f1_negative = f1_score(new_test_predictions_df['label'], new_test_predictions_df['predicted_label'], pos_label=0)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(new_test_predictions_df['label'], new_test_predictions_df['predicted_label'])\n",
    "\n",
    "# Calculate the AUC for males (Geslacht = 1)\n",
    "auc_male = roc_auc_score(new_test_predictions_df[new_test_predictions_df['Geslacht'] == 1]['label'], new_test_predictions_df[new_test_predictions_df['Geslacht'] == 1]['predicted_label'])\n",
    "\n",
    "# Calculate the AUC for females (Geslacht = 0)\n",
    "auc_female = roc_auc_score(new_test_predictions_df[new_test_predictions_df['Geslacht'] == 0]['label'], new_test_predictions_df[new_test_predictions_df['Geslacht'] == 0]['predicted_label'])\n",
    "\n",
    "\n",
    "# Print the calculated metrics separately for both classes\n",
    "print(f\"Accuracy (Overall): {accuracy_positive:.4f}\")\n",
    "print(f\"Precision (Positive): {precision_positive:.4f}\")\n",
    "print(f\"Recall (Positive): {recall_positive:.4f}\")\n",
    "print(f\"F1 Score (Positive): {f1_positive:.4f}\")\n",
    "print(f\"Precision (Negative): {precision_negative:.4f}\")\n",
    "print(f\"Recall (Negative): {recall_negative:.4f}\")\n",
    "print(f\"F1 Score (Negative): {f1_negative:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"AUC (Male): {auc_male:.4f}\")\n",
    "print(f\"AUC (Female): {auc_female:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82da8a10-932e-43a6-bc01-f1da622d1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix for the entire dataset\n",
    "cm = confusion_matrix(new_test_predictions_df['label'], new_test_predictions_df['predicted_label'])\n",
    "\n",
    "# Calculate TPR and FPR for male (Geslacht = 1)\n",
    "male_indices = new_test_predictions_df['Geslacht'] == 1\n",
    "cm_male = confusion_matrix(new_test_predictions_df[male_indices]['label'], new_test_predictions_df[male_indices]['predicted_label'])\n",
    "\n",
    "tpr_male = cm_male[1, 1] / (cm_male[1, 0] + cm_male[1, 1])\n",
    "fpr_male = cm_male[0, 1] / (cm_male[0, 0] + cm_male[0, 1])\n",
    "\n",
    "# Calculate TPR and FPR for female (Geslacht = 0)\n",
    "female_indices = new_test_predictions_df['Geslacht'] == 0\n",
    "cm_female = confusion_matrix(new_test_predictions_df[female_indices]['label'], new_test_predictions_df[female_indices]['predicted_label'])\n",
    "\n",
    "tpr_female = cm_female[1, 1] / (cm_female[1, 0] + cm_female[1, 1])\n",
    "fpr_female = cm_female[0, 1] / (cm_female[0, 0] + cm_female[0, 1])\n",
    "\n",
    "# Print the calculated metrics separately for both classes\n",
    "print(f\"TPR (Male): {tpr_male:.4f}\")\n",
    "print(f\"TPR (Female): {tpr_female:.4f}\")\n",
    "print(f\"FPR (Male): {fpr_male:.4f}\")\n",
    "print(f\"FPR (Female): {fpr_female:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04181989-96b6-4551-a9ab-ab7d18f755b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_predictions_df['combined'] = new_test_predictions_df['Geslacht'].astype(str) + '_' + new_test_predictions_df['label'].astype(str) + '_' + new_test_predictions_df['predicted_label'].astype(str)\n",
    "\n",
    "# Get the count of combinations\n",
    "combination_counts = new_test_predictions_df['combined'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Combined Counts:\")\n",
    "print(combination_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48278161-b380-45a7-b18e-494ed92f2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into first_half and second_half\n",
    "first_half = new_test_predictions_df.iloc[:len(new_test_predictions_df) // 2]\n",
    "second_half = new_test_predictions_df.iloc[len(new_test_predictions_df) // 2:]\n",
    "\n",
    "first_half.reset_index(drop=True, inplace=True)\n",
    "second_half.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Compare the 'predicted_label' values in the two halves and calculate value counts\n",
    "mismatch_counts = (first_half['predicted_label'] != second_half['predicted_label']).value_counts()\n",
    "\n",
    "# Create a DataFrame to display the mismatch counts\n",
    "mismatches = pd.DataFrame({'Mismatches': mismatch_counts})\n",
    "print(mismatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09288d65-c06d-42e1-9b01-63c094050ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test_predictions_df.to_csv('hpc_space/Results/NEW RESULTS/NEW RESULTS/Doc2Vec SVM/Orig/' + 'ROC_SVM5_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4844b8-a8da-469b-af49-2ebb605cccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = first_half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1abd47-a191-4c9c-b938-3699273ea2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "# Calculate accuracy for the positive class (label = 1)\n",
    "accuracy_positive = accuracy_score(first_half['label'], first_half['predicted_label'])\n",
    "\n",
    "# Calculate precision for the positive class (label = 1)\n",
    "precision_positive = precision_score(first_half['label'], first_half['predicted_label'], pos_label=1)\n",
    "\n",
    "# Calculate recall for the positive class (label = 1)\n",
    "recall_positive = recall_score(first_half['label'], first_half['predicted_label'], pos_label=1)\n",
    "\n",
    "# Calculate F1 score for the positive class (label = 1)\n",
    "f1_positive = f1_score(first_half['label'], first_half['predicted_label'], pos_label=1)\n",
    "\n",
    "# Calculate precision for the negative class (label = 0)\n",
    "precision_negative = precision_score(first_half['label'], first_half['predicted_label'], pos_label=0)\n",
    "\n",
    "# Calculate recall for the negative class (label = 0)\n",
    "recall_negative = recall_score(first_half['label'], first_half['predicted_label'], pos_label=0)\n",
    "\n",
    "# Calculate F1 score for the negative class (label = 0)\n",
    "f1_negative = f1_score(first_half['label'], first_half['predicted_label'], pos_label=0)\n",
    "\n",
    "# Calculate the AUC\n",
    "auc = roc_auc_score(first_half['label'], first_half['predicted_label'])\n",
    "\n",
    "# Calculate the AUC for males (Geslacht = 1)\n",
    "auc_male = roc_auc_score(first_half[first_half['Geslacht'] == 1]['label'], first_half[first_half['Geslacht'] == 1]['predicted_label'])\n",
    "\n",
    "# Calculate the AUC for females (Geslacht = 0)\n",
    "auc_female = roc_auc_score(first_half[first_half['Geslacht'] == 0]['label'], first_half[first_half['Geslacht'] == 0]['predicted_label'])\n",
    "\n",
    "\n",
    "# Print the calculated metrics separately for both classes\n",
    "print(f\"Accuracy (Overall): {accuracy_positive:.4f}\")\n",
    "print(f\"Precision (Positive): {precision_positive:.4f}\")\n",
    "print(f\"Recall (Positive): {recall_positive:.4f}\")\n",
    "print(f\"F1 Score (Positive): {f1_positive:.4f}\")\n",
    "print(f\"Precision (Negative): {precision_negative:.4f}\")\n",
    "print(f\"Recall (Negative): {recall_negative:.4f}\")\n",
    "print(f\"F1 Score (Negative): {f1_negative:.4f}\")\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "print(f\"AUC (Male): {auc_male:.4f}\")\n",
    "print(f\"AUC (Female): {auc_female:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba29d2b-7525-4c78-9cd4-b813c0e2e371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix for the entire dataset\n",
    "cm = confusion_matrix(first_half['label'], first_half['predicted_label'])\n",
    "\n",
    "# Calculate TPR and FPR for male (Geslacht = 1)\n",
    "male_indices = first_half['Geslacht'] == 1\n",
    "cm_male = confusion_matrix(first_half[male_indices]['label'], first_half[male_indices]['predicted_label'])\n",
    "\n",
    "tpr_male = cm_male[1, 1] / (cm_male[1, 0] + cm_male[1, 1])\n",
    "fpr_male = cm_male[0, 1] / (cm_male[0, 0] + cm_male[0, 1])\n",
    "\n",
    "# Calculate TPR and FPR for female (Geslacht = 0)\n",
    "female_indices = first_half['Geslacht'] == 0\n",
    "cm_female = confusion_matrix(first_half[female_indices]['label'], first_half[female_indices]['predicted_label'])\n",
    "\n",
    "tpr_female = cm_female[1, 1] / (cm_female[1, 0] + cm_female[1, 1])\n",
    "fpr_female = cm_female[0, 1] / (cm_female[0, 0] + cm_female[0, 1])\n",
    "\n",
    "# Print the calculated metrics separately for both classes\n",
    "print(f\"TPR (Male): {tpr_male:.4f}\")\n",
    "print(f\"TPR (Female): {tpr_female:.4f}\")\n",
    "print(f\"FPR (Male): {fpr_male:.4f}\")\n",
    "print(f\"FPR (Female): {fpr_female:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77206e2-95df-44da-98b6-f0f5c0336778",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_half['combined'] = first_half['Geslacht'].astype(str) + '_' + first_half['label'].astype(str) + '_' + first_half['predicted_label'].astype(str)\n",
    "\n",
    "# Get the count of combinations\n",
    "combination_counts = first_half['combined'].value_counts()\n",
    "\n",
    "# Print the counts\n",
    "print(\"Combined Counts:\")\n",
    "print(combination_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38962e4-0eda-4a97-b488-c70233d3cd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e83b5c-1a08-4058-97d5-9fe6019a0265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vrabiasKernelJoppe",
   "language": "python",
   "name": "vrabiaskerneljoppe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
